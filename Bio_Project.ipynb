{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bio-Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJ/WSeMreBvWgst2qnGXew",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khalil753/Bio-Project/blob/master/Bio_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TB6APJxeI-1",
        "colab_type": "text"
      },
      "source": [
        "# **Bio Project**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYJ0_0_prDYI",
        "colab_type": "text"
      },
      "source": [
        "## Why Fastai?\n",
        "\n",
        "This project was originally to be done with TensorFlow, however, I ended up doing it using Fastai. Here's the reasons why:\n",
        "\n",
        "1.   It's built on top of Pytorch: Pytorch feels much more pythonic than TF, meaning that the intuitions I have on how to write in Python, extrapolate more naturally to Pytorch. Fastai being built on top of Pytorch inherits this quality.\n",
        "\n",
        "2.   Fastai does the preprocessing and training loop for you: This allows you to focus on the important parts of your project withouth having to worry about more superfluos design choices. Also, whenever Fastai could make a good design choice for the user they did so, which means the preprocessing and training loop are higly optimized.\n",
        "\n",
        "3.   They have high quaity leraning resources: On top of creating the library, they also created a [course](https://course.fast.ai/) in which they cover basic topics from deep learning and explain how to use th library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1sw-AAqwNXY",
        "colab_type": "text"
      },
      "source": [
        "## Jupyter notebooks:\n",
        "\n",
        "1.  Data2Images: The datasets provided were saved as numpy arrays, but to make use of Fastai's image preprocessing the datasets must be stored as images in an Imagenet style folder structure. This notebook creates the desired folders and saves the arrays as png images,\n",
        "\n",
        "2.  DirtyTraining: The model (a resnet 18) was trained using all of the traning samples. Then, the loss of each training sample is calculated; losses below 0.5 are tagged as clean.\n",
        "\n",
        "3.  CleanTraining: A second model is trained, this time using only the training samples tagged as clean. This second model was used to reasses which training samples were clean. A third model was trained using these last clean training samples.\n",
        "\n",
        "4.  PerformanceEval: The accuracies of the three models are compared.\n",
        "\n",
        "5.  Deterministic2Bayesian: This notebook contains classes and functions I wrote when I still hoped to use Bayesian neural nets to calculate uncertanties. In the end, they were not used throughout the project, nevertheless, I leave them to show the work done on the matter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjoI7XjMcqgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}