{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bio-Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPg/yL08JIL4E6Emx/U/ef+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khalil753/Bio-Project/blob/master/Bio_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TB6APJxeI-1",
        "colab_type": "text"
      },
      "source": [
        "# **Bio Project**\n",
        "\n",
        "\n",
        "Adrián López\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYJ0_0_prDYI",
        "colab_type": "text"
      },
      "source": [
        "## Why Fastai?\n",
        "\n",
        "This project was originally to be done with TensorFlow, however, I ended up doing it using Fastai. Here's the reasons why:\n",
        "\n",
        "1.   It's built on top of Pytorch: Pytorch feels much more pythonic than TF, meaning that the intuitions I have on how to write in Python, extrapolate more naturally to Pytorch. Fastai being built on top of Pytorch inherits this quality.\n",
        "\n",
        "2.   Fastai does the preprocessing and training loop for you: This allows you to focus on the important parts of your project withouth having to worry about more superfluos design choices. Also, whenever Fastai could make a good design choice for the user they did so, which means the preprocessing and training loop are higly optimized.\n",
        "\n",
        "3.   They have high quaity leraning resources: On top of creating the library, they also created a [course](https://course.fast.ai/) in which they cover basic topics from deep learning and explain how to use th library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1sw-AAqwNXY",
        "colab_type": "text"
      },
      "source": [
        "## Jupyter notebooks:\n",
        "\n",
        "1.  [Data2Images](https://colab.research.google.com/github/khalil753/Bio-Project/blob/master/1_Data2Images.ipynb): The datasets provided were saved as numpy arrays, but to make use of Fastai's image preprocessing the datasets must be stored as images in an Imagenet style folder structure. This notebook creates the desired folders and saves the arrays as png images,\n",
        "\n",
        "2.  [DirtyTraining](https://colab.research.google.com/github/khalil753/Bio-Project/blob/master/2_DirtyTraining.ipynb): The model (a resnet 18) was trained using all of the traning samples. Then, the loss of each training sample is calculated; losses below 0.5 are tagged as clean.\n",
        "\n",
        "3.  [CleanTraining](https://colab.research.google.com/github/khalil753/Bio-Project/blob/master/3_CleanTraining.ipynb): A second model is trained, this time using only the training samples tagged as clean. This second model was used to reasses which training samples were clean. A third model was trained using these last clean training samples.\n",
        "\n",
        "4.  [PerformanceEval](https://colab.research.google.com/github/khalil753/Bio-Project/blob/master/4_PerformanceEval.ipynb): The Performanc of the three models is compared.\n",
        "\n",
        "5.  [Deterministic2Bayesian](https://colab.research.google.com/github/khalil753/Bio-Project/blob/master/Deterministic2Bayesian.ipynb): This notebook contains classes and functions I wrote when I still hoped to use Bayesian neural nets to calculate uncertanties. In the end, they were not used throughout the project, nevertheless, I leave them to show the work done on the matter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mNLfT508iqH",
        "colab_type": "text"
      },
      "source": [
        "## Conclusions\n",
        "\n",
        "*  Given that the loss histogram had the desired shape, the crossentropy loss seems to be a valid measure of the uncertanty of the model. To confirm its validity, the high loss sampels could be shown to a medical professional to determine whether high crossentropy loss indeed corresponds to misslabeled samples.\n",
        "\n",
        "*  By cleaning the training dataset a slight increase in accuracy was observed. The second round of cleaning didn't show any significant increase on accuracy. \n",
        "\n",
        "*  The two different cleaning rounds had different cleaning rates.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjoI7XjMcqgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}